# PIPELINE DEFINITION
# Name: train-workflow
components:
  comp-generate-candidates:
    executorLabel: exec-generate-candidates
    inputDefinitions:
      artifacts:
        item_df_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        item_input_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        user_df_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        user_input_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-load-data-from-feast:
    executorLabel: exec-load-data-from-feast
    outputDefinitions:
      artifacts:
        interaction_df_output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        item_df_output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        neg_interaction_df_output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        user_df_output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      artifacts:
        interaction_df_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        item_df_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        neg_interaction_df_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        user_df_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        item_output_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        user_output_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-generate-candidates:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - generate_candidates
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef generate_candidates(item_input_model: Input[Model], user_input_model:\
          \ Input[Model], item_df_input: Input[Dataset], user_df_input: Input[Dataset]):\n\
          \    from feast import FeatureStore\n    from feast.data_source import PushMode\n\
          \    from models.data_util import data_preproccess\n    from models.user_tower\
          \ import UserTower\n    from models.item_tower import ItemTower\n    import\
          \ pandas as pd\n    import numpy as np\n    from datetime import datetime\n\
          \    import torch\n    store = FeatureStore(repo_path=\"feature_repo/\"\
          )\n\n    item_encoder = ItemTower()\n    user_encoder = UserTower()\n  \
          \  item_encoder.load_state_dict(torch.load(item_input_model.path))\n   \
          \ user_encoder.load_state_dict(torch.load(user_input_model.path))\n    item_encoder.eval()\n\
          \    user_encoder.eval()\n    # load item and user dataframes\n    item_df\
          \ = pd.read_parquet(item_df_input.path)\n    user_df = pd.read_parquet(user_df_input.path)\n\
          \n    # Create a new table to be push to the online store\n    item_embed_df\
          \ = item_df[['item_id']].copy()\n    user_embed_df = user_df[['user_id']].copy()\n\
          \n    # Encode the items and users\n    item_embed_df['embedding'] = item_encoder(**data_preproccess(item_df)).detach().numpy().tolist()\n\
          \    user_embed_df['embedding'] = user_encoder(**data_preproccess(user_df)).detach().numpy().tolist()\n\
          \n    # Add the currnet timestamp\n    item_embed_df['event_timestamp']\
          \ = datetime.now()\n    user_embed_df['event_timestamp'] = datetime.now()\n\
          \n    # Push the new embedding to the offline and online store\n    store.push('item_embed_push_source',\
          \ item_embed_df, to=PushMode.ONLINE)\n    store.push('user_embed_push_source',\
          \ user_embed_df, to=PushMode.ONLINE)\n\n    # Materilize the online store\n\
          \    store.materialize_incremental(datetime.now(), feature_views=['item_embedding',\
          \ 'user_items', 'item_features'])\n\n    # Calculate user recommendations\
          \ for each user\n    item_embedding_view = 'item_embedding'\n    k = 64\n\
          \    item_recommendation = []\n    for user_embed in user_embed_df['embedding']:\n\
          \        item_recommendation.append(\n            store.retrieve_online_documents(\n\
          \                query=user_embed,\n                top_k=k,\n         \
          \       features=[f'{item_embedding_view}:item_id']\n                # features=[f'{item_embedding_view}:item_id',\
          \ f'{item_embedding_view}:embeddings']\n            ).to_df()['item_id'].to_list()\n\
          \        )\n\n    # Pushing the calculated items to the online store\n \
          \   user_items_df = user_embed_df[['user_id']].copy()\n    user_items_df['event_timestamp']\
          \ = datetime.now()\n    user_items_df['top_k_item_ids'] = item_recommendation\n\
          \n    store.push('user_items_push_source', user_items_df, to=PushMode.ONLINE)\n\
          \n"
        image: quay.io/ecosystem-appeng/rec-sys-app:0.0.18
    exec-load-data-from-feast:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_data_from_feast
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'psycopg2' &&\
          \ \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_data_from_feast(item_df_output: Output[Dataset], user_df_output:\
          \ Output[Dataset], interaction_df_output: Output[Dataset], neg_interaction_df_output:\
          \ Output[Dataset]):\n    from feast import FeatureStore\n    from datetime\
          \ import datetime\n    import pandas as pd\n    import os\n    import psycopg2\n\
          \    from sqlalchemy import create_engine, text\n    print(f'MyTest: {os.listdir(\"\
          .\")}')\n    print(f'MyTest2: {os.listdir(\"feature_repo/\")}')\n    print(f'MyTest3:\
          \ {os.listdir(\"feature_repo/secrets\")}')\n    store = FeatureStore(repo_path=\"\
          feature_repo/\")\n    # load feature services\n    item_service = store.get_feature_service(\"\
          item_service\")\n    user_service = store.get_feature_service(\"user_service\"\
          )\n    interaction_service = store.get_feature_service(\"interaction_service\"\
          )\n    neg_interactions_service = store.get_feature_service('neg_interaction_service')\n\
          \n    num_users = 1_000\n    n_items = 5_000\n\n    user_ids = list(range(1,\
          \ num_users+ 1))\n    item_ids = list(range(1, n_items+ 1))\n\n    # select\
          \ which items to use for the training\n    item_entity_df = pd.DataFrame.from_dict(\n\
          \        {\n            'item_id': item_ids,\n            'event_timestamp':\
          \ [datetime(2025, 1, 1)] * len(item_ids) \n        }\n    )\n    # select\
          \ which users to use for the training\n    user_entity_df = pd.DataFrame.from_dict(\n\
          \        {\n            'user_id': user_ids,\n            'event_timestamp':\
          \ [datetime(2025, 1, 1)] * len(user_ids) \n        }\n    )\n    # Select\
          \ which item-user interactions to use for the training\n    item_user_interactions_df\
          \ = pd.read_parquet('./feature_repo/data/interactions_item_user_ids.parquet')\n\
          \    item_user_neg_interactions_df = pd.read_parquet('./feature_repo/data/neg_interactions_item_user_ids.parquet')\n\
          \    item_user_interactions_df['event_timestamp'] = datetime(2025, 1, 1)\n\
          \    item_user_neg_interactions_df['event_timestamp'] = datetime(2025, 1,\
          \ 1)\n\n    # retrive datasets for training\n    item_df = store.get_historical_features(entity_df=item_entity_df,\
          \ features=item_service).to_df()\n    user_df = store.get_historical_features(entity_df=user_entity_df,\
          \ features=user_service).to_df()\n    interaction_df = store.get_historical_features(entity_df=item_user_interactions_df,\
          \ features=interaction_service).to_df()\n    neg_interaction_df = store.get_historical_features(entity_df=item_user_neg_interactions_df,\
          \ features=neg_interactions_service).to_df()\n\n    uri = os.getenv('uri',\
          \ None)\n    engine = create_engine(uri)\n\n    def table_exists(engine,\
          \ table_name):\n        query = text(\"SELECT COUNT(*) FROM information_schema.tables\
          \ WHERE table_name = :table_name\")\n        with engine.connect() as connection:\n\
          \            result = connection.execute(query, {\"table_name\": table_name}).scalar()\n\
          \            return result > 0\n\n    if table_exists(engine, 'stream_interaction_negetive'):\n\
          \        query_negative = 'SELECT * FROM stream_interaction_negetive'\n\
          \        stream_negetive_inter_df = pd.read_sql(query_negative, engine).rename(columns={'timestamp':'event_timestamp'})\n\
          \n        neg_interaction_df = pd.concat([neg_interaction_df, stream_negetive_inter_df],\
          \ axis=0)\n\n    if table_exists(engine, 'stream_interaction_positive'):\n\
          \        query_positive = 'SELECT * FROM stream_interaction_positive'\n\
          \        stream_positive_inter_df = pd.read_sql(query_positive, engine).rename(columns={'timestamp':'event_timestamp'})\n\
          \n        interaction_df = pd.concat([interaction_df, stream_positive_inter_df],\
          \ axis=0)\n    # Pass artifacts\n    item_df.to_parquet(item_df_output.path)\n\
          \    user_df.to_parquet(user_df_output.path)\n    interaction_df.to_parquet(interaction_df_output.path)\n\
          \    neg_interaction_df.to_parquet(neg_interaction_df_output.path)\n\n \
          \   item_df_output.metadata['format'] = 'parquet'\n    user_df_output.metadata['format']\
          \ = 'parquet'\n    interaction_df_output.metadata['format'] = 'parquet'\n\
          \    neg_interaction_df_output.metadata['format'] = 'parquet'\n\n"
        image: quay.io/ecosystem-appeng/rec-sys-app:0.0.18
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(item_df_input: Input[Dataset], user_df_input: Input[Dataset],\
          \ interaction_df_input: Input[Dataset], neg_interaction_df_input:Input[Dataset],\
          \ item_output_model: Output[Model], user_output_model: Output[Model]):\n\
          \    from models.user_tower import UserTower\n    from models.item_tower\
          \ import ItemTower\n    from models.train_two_tower import train_two_tower\n\
          \    import pandas as pd\n    import torch\n    dim = 64\n\n    item_df\
          \ = pd.read_parquet(item_df_input.path)\n    user_df = pd.read_parquet(user_df_input.path)\n\
          \    interaction_df = pd.read_parquet(interaction_df_input.path)\n    neg_interaction_df\
          \ = pd.read_parquet(neg_interaction_df_input.path)\n\n    item_encoder =\
          \ ItemTower(dim)\n    user_encoder = UserTower(dim)\n    train_two_tower(item_encoder,\
          \ user_encoder, item_df, user_df, interaction_df, neg_interaction_df)\n\n\
          \    torch.save(item_encoder.state_dict(), item_output_model.path)\n   \
          \ torch.save(user_encoder.state_dict(), user_output_model.path)\n    item_output_model.metadata['framework']\
          \ = 'pytorch'\n    user_output_model.metadata['framework'] = 'pytorch'\n\
          \n"
        image: quay.io/ecosystem-appeng/rec-sys-app:0.0.18
pipelineInfo:
  name: train-workflow
root:
  dag:
    tasks:
      generate-candidates:
        cachingOptions: {}
        componentRef:
          name: comp-generate-candidates
        dependentTasks:
        - load-data-from-feast
        - train-model
        inputs:
          artifacts:
            item_df_input:
              taskOutputArtifact:
                outputArtifactKey: item_df_output
                producerTask: load-data-from-feast
            item_input_model:
              taskOutputArtifact:
                outputArtifactKey: item_output_model
                producerTask: train-model
            user_df_input:
              taskOutputArtifact:
                outputArtifactKey: user_df_output
                producerTask: load-data-from-feast
            user_input_model:
              taskOutputArtifact:
                outputArtifactKey: user_output_model
                producerTask: train-model
        taskInfo:
          name: generate-candidates
      load-data-from-feast:
        cachingOptions: {}
        componentRef:
          name: comp-load-data-from-feast
        taskInfo:
          name: load-data-from-feast
      train-model:
        cachingOptions: {}
        componentRef:
          name: comp-train-model
        dependentTasks:
        - load-data-from-feast
        inputs:
          artifacts:
            interaction_df_input:
              taskOutputArtifact:
                outputArtifactKey: interaction_df_output
                producerTask: load-data-from-feast
            item_df_input:
              taskOutputArtifact:
                outputArtifactKey: item_df_output
                producerTask: load-data-from-feast
            neg_interaction_df_input:
              taskOutputArtifact:
                outputArtifactKey: neg_interaction_df_output
                producerTask: load-data-from-feast
            user_df_input:
              taskOutputArtifact:
                outputArtifactKey: user_df_output
                producerTask: load-data-from-feast
        taskInfo:
          name: train-model
schemaVersion: 2.1.0
sdkVersion: kfp-2.11.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-generate-candidates:
          secretAsEnv:
          - keyToEnv:
            - envVar: uri
              secretKey: uri
            - envVar: DB_PASSWORD
              secretKey: password
            secretName: cluster-sample-app
          secretAsVolume:
          - mountPath: /app/feature_repo/secrets
            optional: false
            secretName: feast-feast-edb-rec-sys-registry-tls
        exec-load-data-from-feast:
          secretAsEnv:
          - keyToEnv:
            - envVar: uri
              secretKey: uri
            - envVar: DB_PASSWORD
              secretKey: password
            secretName: cluster-sample-app
          secretAsVolume:
          - mountPath: /app/feature_repo/secrets
            optional: false
            secretName: feast-feast-edb-rec-sys-registry-tls

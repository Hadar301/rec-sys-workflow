# PIPELINE DEFINITION
# Name: train-workflow
components:
  comp-generate-candidates:
    executorLabel: exec-generate-candidates
    inputDefinitions:
      artifacts:
        item_df_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        item_input_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        models_definition_input:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        user_df_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        user_input_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-load-data-from-feast:
    executorLabel: exec-load-data-from-feast
    outputDefinitions:
      artifacts:
        interaction_df_output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        item_df_output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        user_df_output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      artifacts:
        interaction_df_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        item_df_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        user_df_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        item_output_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        models_definition_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        user_output_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-generate-candidates:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - generate_candidates
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef generate_candidates(item_input_model: Input[Model], user_input_model:\
          \ Input[Model], item_df_input: Input[Dataset], user_df_input: Input[Dataset],\
          \ models_definition_input: Input[Artifact]):\n    from feast import FeatureStore\n\
          \    from feast.data_source import PushMode\n    from models.data_util import\
          \ data_preproccess\n    from models.entity_tower import EntityTower\n  \
          \  import pandas as pd\n    import numpy as np\n    from datetime import\
          \ datetime\n    import torch\n    import subprocess\n    import json\n\n\
          \    with open(models_definition_input.path, 'r') as f:\n        models_definition\
          \ :dict = json.load(f)\n\n    result = subprocess.run(\n        [\"/bin/bash\"\
          , \"-c\", \"ls && ./entry_point.sh\"],\n        capture_output=True,  #\
          \ Capture stdout and stderr\n        text=True,           # Return output\
          \ as strings (not bytes)\n        # check=True           # Raise an error\
          \ if the command fails\n    )\n\n    # Print the stdout\n    print(\"Standard\
          \ Output:\")\n    print(result.stdout)\n\n    # Print the stderr (if any)\n\
          \    print(\"Standard Error:\")\n    print(result.stderr)\n    with open('feature_repo/feature_store.yaml',\
          \ 'r') as file:\n        print(file.read())\n\n    store = FeatureStore(repo_path=\"\
          feature_repo/\")\n\n    # device = torch.device('cuda' if torch.cuda.is_available()\
          \ else 'cpu')\n    device = torch.device('cpu')\n    item_encoder = EntityTower(models_definition['items_num_numerical'],\
          \ models_definition['items_num_categorical'])\n    user_encoder = EntityTower(models_definition['users_num_numerical'],\
          \ models_definition['users_num_categorical'])\n    item_encoder.load_state_dict(torch.load(item_input_model.path))\n\
          \    user_encoder.load_state_dict(torch.load(user_input_model.path))\n \
          \   item_encoder.to(device)\n    user_encoder.to(device)\n    item_encoder.eval()\n\
          \    user_encoder.eval()\n    # load item and user dataframes\n    item_df\
          \ = pd.read_parquet(item_df_input.path)\n    user_df = pd.read_parquet(user_df_input.path)\n\
          \n    # Create a new table to be push to the online store\n    item_embed_df\
          \ = item_df[['item_id']].copy()\n    user_embed_df = user_df[['user_id']].copy()\n\
          \n    # Encode the items and users\n    proccessed_items = data_preproccess(item_df)\n\
          \    proccessed_users = data_preproccess(user_df)\n    # Move tensors to\
          \ device\n    proccessed_items = {key: value.to(device) if type(value) ==\
          \ torch.Tensor else value for key, value in proccessed_items.items()}\n\
          \    proccessed_users = {key: value.to(device) if type(value) == torch.Tensor\
          \ else value for key, value in proccessed_users.items()}\n    item_embed_df['embedding']\
          \ = item_encoder(**proccessed_items).detach().numpy().tolist()\n    user_embed_df['embedding']\
          \ = user_encoder(**proccessed_users).detach().numpy().tolist()\n\n    #\
          \ Add the currnet timestamp\n    item_embed_df['event_timestamp'] = datetime.now()\n\
          \    user_embed_df['event_timestamp'] = datetime.now()\n\n    # Push the\
          \ new embedding to the offline and online store\n    store.push('item_embed_push_source',\
          \ item_embed_df, to=PushMode.ONLINE, allow_registry_cache=False)\n    store.push('user_embed_push_source',\
          \ user_embed_df, to=PushMode.ONLINE, allow_registry_cache=False)\n\n   \
          \ # Store the embedding of text features for search by text\n    item_text_features_embed\
          \ = item_df[['item_id']].copy()\n    # item_text_features_embed['product_name']\
          \ = proccessed_items['text_features'].detach()[:, 0, :].numpy().tolist()\n\
          \    item_text_features_embed['about_product_embedding'] = proccessed_items['text_features'].detach()[:,\
          \ 1, :].numpy().tolist()\n    item_text_features_embed['event_timestamp']\
          \ = datetime.now()\n\n    store.push('item_textual_features_embed', item_text_features_embed,\
          \ to=PushMode.ONLINE, allow_registry_cache=False)\n\n    # Materilize the\
          \ online store\n    store.materialize_incremental(datetime.now(), feature_views=['item_embedding',\
          \ 'user_items', 'item_features', 'item_textual_features_embed'])\n\n   \
          \ # Calculate user recommendations for each user\n    item_embedding_view\
          \ = 'item_embedding'\n    k = 64\n    item_recommendation = []\n    for\
          \ user_embed in user_embed_df['embedding']:\n        item_recommendation.append(\n\
          \            store.retrieve_online_documents(\n                query=user_embed,\n\
          \                top_k=k,\n                features=[f'{item_embedding_view}:item_id']\n\
          \            ).to_df()['item_id'].to_list()\n        )\n\n    # Pushing\
          \ the calculated items to the online store\n    user_items_df = user_embed_df[['user_id']].copy()\n\
          \    user_items_df['event_timestamp'] = datetime.now()\n    user_items_df['top_k_item_ids']\
          \ = item_recommendation\n\n    store.push('user_items_push_source', user_items_df,\
          \ to=PushMode.ONLINE, allow_registry_cache=False)\n\n"
        env:
        - name: FEAST_PROJECT_NAME
          value: feast_edb_rec_sys
        - name: FEAST_REGISTRY_URL
          value: feast-feast-edb-rec-sys-registry.rec-sys.svc.cluster.local
        image: quay.io/ecosystem-appeng/rec-sys-app:0.0.41
    exec-load-data-from-feast:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_data_from_feast
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'psycopg2' &&\
          \ \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_data_from_feast(item_df_output: Output[Dataset], user_df_output:\
          \ Output[Dataset], interaction_df_output: Output[Dataset]):\n    from feast\
          \ import FeatureStore\n    from datetime import datetime\n    import pandas\
          \ as pd\n    import os\n    import psycopg2\n    from sqlalchemy import\
          \ create_engine, text\n    import subprocess\n\n    result = subprocess.run(\n\
          \        [\"/bin/bash\", \"-c\", \"ls && ./entry_point.sh\"],\n        capture_output=True,\
          \  # Capture stdout and stderr\n        text=True,           # Return output\
          \ as strings (not bytes)\n    )\n\n    # Print the stdout\n    print(\"\
          Standard Output:\")\n    print(result.stdout)\n\n    # Print the stderr\
          \ (if any)\n    print(\"Standard Error:\")\n    print(result.stderr)\n\n\
          \    with open('feature_repo/feature_store.yaml', 'r') as file:\n      \
          \  print(file.read())\n    store = FeatureStore(repo_path=\"feature_repo/\"\
          )\n    store.refresh_registry()\n    print('registry refreshed')\n    #\
          \ load feature services\n    item_service = store.get_feature_service(\"\
          item_service\")\n    user_service = store.get_feature_service(\"user_service\"\
          )\n    interaction_service = store.get_feature_service(\"interaction_service\"\
          )\n\n    users_ids = pd.read_parquet('./feature_repo/data/recommendation_interactions.parquet')\n\
          \    user_ids = users_ids['user_id'].unique().tolist()\n    item_ids = users_ids['item_id'].unique().tolist()\n\
          \n    # select which items to use for the training\n    item_entity_df =\
          \ pd.DataFrame.from_dict(\n        {\n            'item_id': item_ids,\n\
          \            'event_timestamp': [datetime(2025, 1, 1)] * len(item_ids)\n\
          \        }\n    )\n    # select which users to use for the training\n  \
          \  user_entity_df = pd.DataFrame.from_dict(\n        {\n            'user_id':\
          \ user_ids,\n            'event_timestamp': [datetime(2025, 1, 1)] * len(user_ids)\n\
          \        }\n    )\n    # Select which item-user interactions to use for\
          \ the training\n    item_user_interactions_df = pd.read_parquet('./feature_repo/data/interactions_item_user_ids.parquet')\n\
          \    item_user_interactions_df['event_timestamp'] = datetime(2025, 1, 1)\n\
          \n    # retrive datasets for training\n    item_df = store.get_historical_features(entity_df=item_entity_df,\
          \ features=item_service).to_df()\n    user_df = store.get_historical_features(entity_df=user_entity_df,\
          \ features=user_service).to_df()\n    interaction_df = store.get_historical_features(entity_df=item_user_interactions_df,\
          \ features=interaction_service).to_df()\n\n    uri = os.getenv('uri', None)\n\
          \    engine = create_engine(uri)\n\n    def table_exists(engine, table_name):\n\
          \        query = text(\"SELECT COUNT(*) FROM information_schema.tables WHERE\
          \ table_name = :table_name\")\n        with engine.connect() as connection:\n\
          \            result = connection.execute(query, {\"table_name\": table_name}).scalar()\n\
          \            return result > 0\n\n    if table_exists(engine, 'new_users'):\n\
          \        query_new_users = 'SELECT * FROM new_users'\n        stream_users_df\
          \ = pd.read_sql(query_new_users, engine).rename(columns={'timestamp':'signup_date'})\n\
          \n        user_df = pd.concat([user_df, stream_users_df], axis=0)\n\n  \
          \  if table_exists(engine, 'stream_interaction'):\n        query_positive\
          \ = 'SELECT * FROM stream_interaction'\n        stream_positive_inter_df\
          \ = pd.read_sql(query_positive, engine).rename(columns={'timestamp':'event_timestamp'})\n\
          \n        interaction_df = pd.concat([interaction_df, stream_positive_inter_df],\
          \ axis=0)\n\n    # Pass artifacts\n    item_df.to_parquet(item_df_output.path)\n\
          \    user_df.to_parquet(user_df_output.path)\n    interaction_df.to_parquet(interaction_df_output.path)\n\
          \n    item_df_output.metadata['format'] = 'parquet'\n    user_df_output.metadata['format']\
          \ = 'parquet'\n    interaction_df_output.metadata['format'] = 'parquet'\n\
          \n"
        env:
        - name: FEAST_PROJECT_NAME
          value: feast_edb_rec_sys
        - name: FEAST_REGISTRY_URL
          value: feast-feast-edb-rec-sys-registry.rec-sys.svc.cluster.local
        image: quay.io/ecosystem-appeng/rec-sys-app:0.0.41
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(item_df_input: Input[Dataset], user_df_input: Input[Dataset],\
          \ interaction_df_input: Input[Dataset], item_output_model: Output[Model],\
          \ user_output_model: Output[Model], models_definition_output: Output[Artifact]):\n\
          \    from models.train_two_tower import create_and_train_two_tower\n   \
          \ import pandas as pd\n    import torch\n    import json\n\n    item_df\
          \ = pd.read_parquet(item_df_input.path)\n    user_df = pd.read_parquet(user_df_input.path)\n\
          \    interaction_df = pd.read_parquet(interaction_df_input.path)\n\n   \
          \ item_encoder, user_encoder, models_definition= create_and_train_two_tower(item_df,\
          \ user_df, interaction_df, return_model_definition=True)\n\n    torch.save(item_encoder.state_dict(),\
          \ item_output_model.path)\n    torch.save(user_encoder.state_dict(), user_output_model.path)\n\
          \    item_output_model.metadata['framework'] = 'pytorch'\n    user_output_model.metadata['framework']\
          \ = 'pytorch'\n    with open(models_definition_output.path, 'w') as f:\n\
          \        json.dump(models_definition, f)\n\n"
        image: quay.io/ecosystem-appeng/rec-sys-app:0.0.41
pipelineInfo:
  name: train-workflow
root:
  dag:
    tasks:
      generate-candidates:
        cachingOptions: {}
        componentRef:
          name: comp-generate-candidates
        dependentTasks:
        - load-data-from-feast
        - train-model
        inputs:
          artifacts:
            item_df_input:
              taskOutputArtifact:
                outputArtifactKey: item_df_output
                producerTask: load-data-from-feast
            item_input_model:
              taskOutputArtifact:
                outputArtifactKey: item_output_model
                producerTask: train-model
            models_definition_input:
              taskOutputArtifact:
                outputArtifactKey: models_definition_output
                producerTask: train-model
            user_df_input:
              taskOutputArtifact:
                outputArtifactKey: user_df_output
                producerTask: load-data-from-feast
            user_input_model:
              taskOutputArtifact:
                outputArtifactKey: user_output_model
                producerTask: train-model
        taskInfo:
          name: generate-candidates
      load-data-from-feast:
        cachingOptions: {}
        componentRef:
          name: comp-load-data-from-feast
        taskInfo:
          name: load-data-from-feast
      train-model:
        cachingOptions: {}
        componentRef:
          name: comp-train-model
        dependentTasks:
        - load-data-from-feast
        inputs:
          artifacts:
            interaction_df_input:
              taskOutputArtifact:
                outputArtifactKey: interaction_df_output
                producerTask: load-data-from-feast
            item_df_input:
              taskOutputArtifact:
                outputArtifactKey: item_df_output
                producerTask: load-data-from-feast
            user_df_input:
              taskOutputArtifact:
                outputArtifactKey: user_df_output
                producerTask: load-data-from-feast
        taskInfo:
          name: train-model
schemaVersion: 2.1.0
sdkVersion: kfp-2.11.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-generate-candidates:
          secretAsEnv:
          - keyToEnv:
            - envVar: uri
              secretKey: uri
            - envVar: DB_PASSWORD
              secretKey: password
            - envVar: DB_HOST
              secretKey: host
            - envVar: DB_NAME
              secretKey: dbname
            - envVar: DB_USER
              secretKey: user
            - envVar: DB_PORT
              secretKey: port
            secretName: cluster-sample-app
          secretAsVolume:
          - mountPath: /app/feature_repo/secrets
            optional: false
            secretName: feast-feast-edb-rec-sys-registry-tls
        exec-load-data-from-feast:
          secretAsEnv:
          - keyToEnv:
            - envVar: uri
              secretKey: uri
            - envVar: DB_PASSWORD
              secretKey: password
            - envVar: DB_HOST
              secretKey: host
            - envVar: DB_NAME
              secretKey: dbname
            - envVar: DB_USER
              secretKey: user
            - envVar: DB_PORT
              secretKey: port
            secretName: cluster-sample-app
          secretAsVolume:
          - mountPath: /app/feature_repo/secrets
            optional: false
            secretName: feast-feast-edb-rec-sys-registry-tls
